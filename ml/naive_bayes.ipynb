{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "1. Naive Bayes is a probabilistic classifier that uses conditional probability to classify the classes.\n",
    "2. The core formula is:  \n",
    "   $$\n",
    "   P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}\n",
    "   $$\n",
    "3. In this implementation, we have 3 features, 10 samples, and 2 classes.\n",
    "4. The formula for calculating \\( P(y = 0|X) \\) is:  \n",
    "   $$\n",
    "   P(y = 0|X) = \\frac{P(x_1|y = 0) \\cdot P(x_2|y = 0) \\cdot \\dots \\cdot P(x_{10}|y = 0) \\cdot P(y = 0)}{P(X)}\n",
    "   $$\n",
    "5. Similarly, the formula for calculating \\( P(y = 1|X) \\) is:  \n",
    "   $$\n",
    "   P(y = 1|X) = \\frac{P(x_1|y = 1) \\cdot P(x_2|y = 1) \\cdot \\dots \\cdot P(x_{10}|y = 1) \\cdot P(y = 1)}{P(X)}\n",
    "   $$\n",
    "6. Applying the **log transformation** and finding the **Maximum Likelihood Estimation (MLE)**:  \n",
    "   $$\n",
    "   \\arg\\max_y \\log P(X|y) \\cdot P(y) / P(X)\n",
    "   $$\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. **Feature Independence**:  \n",
    "   The features \\( x_1, x_2, ..., x_n \\) are assumed to be conditionally independent given the class \\( y \\).  \n",
    "   This is why:  \n",
    "   $$\n",
    "   P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot \\dots \\cdot P(x_n|y)\n",
    "   $$\n",
    "   \n",
    "2. **Feature Distribution**:  \n",
    "   - **Gaussian Naive Bayes**: For continuous features, we assume that for a given value of \\( y \\), the features \\( x_1, x_2, ..., x_n \\) follow a normal (Gaussian) distribution.\n",
    "   \n",
    "## Types of Naive Bayes Models Based on Feature Distribution\n",
    "\n",
    "1. **Gaussian Naive Bayes**:  \n",
    "   Assumes the features follow a Gaussian (normal) distribution.\n",
    "   \n",
    "2. **Bernoulli Naive Bayes**:  \n",
    "   Assumes binary features, i.e., presence/absence of a feature.\n",
    "   \n",
    "3. **Multinomial Naive Bayes**:  \n",
    "   Assumes the features follow a multinomial distribution. Typically used for text classification, where the features are word counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class means:\n",
      " tensor([[-0.6612,  0.7808,  0.4939,  1.3309,  0.4079, -0.1055,  0.1198,  0.2943,\n",
      "         -1.0946,  0.2843],\n",
      "        [ 0.4768, -0.2492,  0.7111, -0.1231, -0.3066,  0.1550, -0.3000, -0.7018,\n",
      "          0.1467,  0.6364]])\n",
      "Class variances:\n",
      " tensor([[9.4736e-04, 2.8718e-01, 1.0667e-01, 1.0123e-03, 1.0996e+00, 2.6818e-02,\n",
      "         7.4482e-01, 2.0912e-01, 2.4093e+00, 5.9047e-01],\n",
      "        [9.4817e-01, 1.6443e+00, 9.4338e-02, 1.2655e+00, 3.7627e-01, 8.0746e-01,\n",
      "         1.6643e+00, 4.9312e-01, 7.6913e-01, 2.0216e-01]])\n",
      "Class priors (P(class)):\n",
      " tensor([0.2000, 0.8000])\n",
      "Test sample: tensor([[-1.3592,  0.1304, -0.8333, -0.7610, -0.1195,  1.3507, -0.0998,  0.5311,\n",
      "          0.9302, -0.1602]])\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example data: 10 samples, 3 features\n",
    "x = torch.randn([10, 10])\n",
    "y = torch.bernoulli(torch.full((10,), 0.7)).int()  # Binary class labels: 0 or 1\n",
    "xtest = torch.randn(1, 10)  # A new test sample (1 x 3 features)\n",
    "\n",
    "# Count the number of unique classes and their frequencies\n",
    "cls_value, cls_count = torch.unique(y, return_counts=True)\n",
    "yclass, yclass_count = torch.unique(y, return_counts=True)\n",
    "\n",
    "n_class = yclass.shape[0]  # Number of unique classes (2 in this case: 0 and 1)\n",
    "n_features = x.shape[1]    # Number of features (3 features in this case)\n",
    "\n",
    "# Initialize containers for class means, variances, and class priors\n",
    "cmean = torch.zeros([n_class, n_features])  # Mean of each feature for each class\n",
    "cvar = torch.zeros([n_class, n_features])   # Variance of each feature for each class\n",
    "ycls = torch.zeros(n_class)  # Class priors (probability of each class)\n",
    "\n",
    "# Calculate class means, variances, and priors\n",
    "for i, cls in enumerate(yclass):\n",
    "    x_cls = x[y == cls]  # Get all samples corresponding to the class\n",
    "    cmean[i] = x_cls.mean(dim=0)  # Mean of features for this class\n",
    "    cvar[i] = x_cls.var(dim=0, unbiased=False)  # Variance of features for this class\n",
    "    ycls[i] = cls.item()  # Store the class label (0 or 1) for each class\n",
    "\n",
    "# Compute class priors (P(class))\n",
    "for i, count in enumerate(cls_count):\n",
    "    ycls_prob = count / y.shape[0]  # P(class)\n",
    "    ycls[i] = ycls_prob\n",
    "\n",
    "print(\"Class means:\\n\", cmean)\n",
    "print(\"Class variances:\\n\", cvar)\n",
    "print(\"Class priors (P(class)):\\n\", ycls)\n",
    "\n",
    "def norm_pdf(x, mu, var):\n",
    "    \"\"\"Compute the normal probability density function.\"\"\"\n",
    "    return torch.exp(-0.5 * ((x - mu) ** 2) / var) / (2 * 3.14 * var) ** 0.5\n",
    "\n",
    "def log(z):\n",
    "    \"\"\"Compute the logarithm of a tensor.\"\"\"\n",
    "    return torch.log(z)\n",
    "\n",
    "def predict(xtest, cmean, cvar, yclass_count, yclass):\n",
    "    \"\"\"\n",
    "    Predict the class of a new sample using Gaussian Naive Bayes.\n",
    "    Arguments:\n",
    "    - xtest: Test sample (1 x n_features)\n",
    "    - cmean: Mean values for each feature per class\n",
    "    - cvar: Variance values for each feature per class\n",
    "    - yclass_count: Count of samples per class (used for P(class))\n",
    "    - yclass: Class labels (0 and 1)\n",
    "\n",
    "    Returns:\n",
    "    - Predicted class\n",
    "    \"\"\"\n",
    "    n_classes = cmean.shape[0]\n",
    "    log_posterior = torch.zeros(n_classes)\n",
    "\n",
    "    # Compute log likelihood and log prior for each class\n",
    "    for i in range(n_classes):\n",
    "        # Calculate the normal PDF for each feature in the test sample for class i\n",
    "        log_likelihood = torch.sum(log(norm_pdf(xtest, cmean[i], cvar[i])))  # Sum of log(P(x_i | class))\n",
    "        log_prior = log(yclass_count[i])  # Log(P(class))\n",
    "        \n",
    "        # Total log posterior (log of P(x | class) * P(class))\n",
    "        log_posterior[i] = log_likelihood + log_prior\n",
    "\n",
    "    # Choose the class with the highest posterior probability\n",
    "    predicted_class = torch.argmax(log_posterior)\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Example of predicting a new sample\n",
    "predicted_class = predict(xtest, cmean, cvar, yclass_count, yclass)\n",
    "print(\"Test sample:\", xtest)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorchifying Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "\n",
    "    def fit(self, xtrain, ytrain):\n",
    "        self.cls_value, self.cls_count = torch.unique(ytrain, return_counts=True)\n",
    "        self.yclass, self.yclass_count = torch.unique(ytrain, return_counts=True)\n",
    "        self.n_class = self.yclass.shape[0]  # Number of unique classes (2 in this case: 0 and 1)\n",
    "        self.n_features = xtrain.shape[1]\n",
    "        self.cmean = torch.zeros([self.n_class, self.n_features])  # Mean of each feature for each class\n",
    "        self.cvar = torch.zeros([self.n_class, self.n_features])   # Variance of each feature for each class\n",
    "        self.ycls = torch.zeros(self.n_class)   \n",
    "\n",
    "        for i, cls in enumerate(self.yclass):\n",
    "            x_cls = xtrain[ytrain == cls]  # Get all samples corresponding to the class\n",
    "            self.cmean[i] = x_cls.mean(dim=0)  # Mean of features for this class\n",
    "            self.cvar[i] = x_cls.var(dim=0, unbiased=False)  # Variance of features for this class\n",
    "            self.ycls[i] = self.cls_count[i].item() / ytrain.shape[0]  # Class prior P(class)\n",
    "        \n",
    "          # Returning self allows method chaining\n",
    "         \n",
    "\n",
    "    def predict(self, xtest):\n",
    "        self.n_classes = self.cmean.shape[0]\n",
    "        log_posterior = torch.zeros(self.n_classes)\n",
    "\n",
    "        # Compute log likelihood and log prior for each class\n",
    "        for i in range(self.n_classes):\n",
    "            # Calculate the normal PDF for each feature in the test sample for class i\n",
    "            log_likelihood = torch.sum(log(norm_pdf(xtest, self.cmean[i], self.cvar[i])))  # Sum of log(P(x_i | class))\n",
    "            log_prior = log(self.yclass_count[i])  # Log(P(class))\n",
    "            \n",
    "            # Total log posterior (log of P(x | class) * P(class))\n",
    "            log_posterior[i] = log_likelihood + log_prior\n",
    "\n",
    "        # Choose the class with the highest posterior probability\n",
    "        predicted_class = torch.argmax(log_posterior)\n",
    "        return predicted_class.item()\n",
    "        \n",
    "\n",
    "    def norm_pdf(self, xtest, mu, var):\n",
    "        return torch.exp(-0.5 * ((xtest - mu) ** 2) / var) / (2 * 3.14 * var) ** 0.5\n",
    "         \n",
    "    def log(z):\n",
    "        return torch.log(z)\n",
    "     \n",
    "\n",
    "\n",
    "obj = NaiveBayes()\n",
    "obj1 = obj.fit(x,y)\n",
    "cls_predict =  obj.predict(xtest)\n",
    "cls_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

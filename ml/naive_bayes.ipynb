{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class means:\n",
      " tensor([[-0.2296,  0.4988,  0.4024],\n",
      "        [-0.0158, -0.5368,  0.1436]])\n",
      "Class variances:\n",
      " tensor([[1.6229, 0.5590, 0.4203],\n",
      "        [1.2260, 0.1312, 0.3073]])\n",
      "Class priors (P(class)):\n",
      " tensor([0.4000, 0.6000])\n",
      "Test sample: tensor([[ 2.2359, -0.0222,  0.3576]])\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example data: 10 samples, 3 features\n",
    "x = torch.randn([10, 3])\n",
    "y = torch.bernoulli(torch.full((10,), 0.7)).int()  # Binary class labels: 0 or 1\n",
    "xtest = torch.randn(1, 3)  # A new test sample (1 x 3 features)\n",
    "\n",
    "# Count the number of unique classes and their frequencies\n",
    "cls_value, cls_count = torch.unique(y, return_counts=True)\n",
    "yclass, yclass_count = torch.unique(y, return_counts=True)\n",
    "\n",
    "n_class = yclass.shape[0]  # Number of unique classes (2 in this case: 0 and 1)\n",
    "n_features = x.shape[1]    # Number of features (3 features in this case)\n",
    "\n",
    "# Initialize containers for class means, variances, and class priors\n",
    "cmean = torch.zeros([n_class, n_features])  # Mean of each feature for each class\n",
    "cvar = torch.zeros([n_class, n_features])   # Variance of each feature for each class\n",
    "ycls = torch.zeros(n_class)  # Class priors (probability of each class)\n",
    "\n",
    "# Calculate class means, variances, and priors\n",
    "for i, cls in enumerate(yclass):\n",
    "    x_cls = x[y == cls]  # Get all samples corresponding to the class\n",
    "    cmean[i] = x_cls.mean(dim=0)  # Mean of features for this class\n",
    "    cvar[i] = x_cls.var(dim=0, unbiased=False)  # Variance of features for this class\n",
    "    ycls[i] = cls.item()  # Store the class label (0 or 1) for each class\n",
    "\n",
    "# Compute class priors (P(class))\n",
    "for i, count in enumerate(cls_count):\n",
    "    ycls_prob = count / y.shape[0]  # P(class)\n",
    "    ycls[i] = ycls_prob\n",
    "\n",
    "print(\"Class means:\\n\", cmean)\n",
    "print(\"Class variances:\\n\", cvar)\n",
    "print(\"Class priors (P(class)):\\n\", ycls)\n",
    "\n",
    "def norm_pdf(x, mu, var):\n",
    "    \"\"\"Compute the normal probability density function.\"\"\"\n",
    "    return torch.exp(-0.5 * ((x - mu) ** 2) / var) / (2 * 3.14 * var) ** 0.5\n",
    "\n",
    "def log(z):\n",
    "    \"\"\"Compute the logarithm of a tensor.\"\"\"\n",
    "    return torch.log(z)\n",
    "\n",
    "def predict(xtest, cmean, cvar, yclass_count, yclass):\n",
    "    \"\"\"\n",
    "    Predict the class of a new sample using Gaussian Naive Bayes.\n",
    "    Arguments:\n",
    "    - xtest: Test sample (1 x n_features)\n",
    "    - cmean: Mean values for each feature per class\n",
    "    - cvar: Variance values for each feature per class\n",
    "    - yclass_count: Count of samples per class (used for P(class))\n",
    "    - yclass: Class labels (0 and 1)\n",
    "\n",
    "    Returns:\n",
    "    - Predicted class\n",
    "    \"\"\"\n",
    "    n_classes = cmean.shape[0]\n",
    "    log_posterior = torch.zeros(n_classes)\n",
    "\n",
    "    # Compute log likelihood and log prior for each class\n",
    "    for i in range(n_classes):\n",
    "        # Calculate the normal PDF for each feature in the test sample for class i\n",
    "        log_likelihood = torch.sum(log(norm_pdf(xtest, cmean[i], cvar[i])))  # Sum of log(P(x_i | class))\n",
    "        log_prior = log(yclass_count[i])  # Log(P(class))\n",
    "        \n",
    "        # Total log posterior (log of P(x | class) * P(class))\n",
    "        log_posterior[i] = log_likelihood + log_prior\n",
    "\n",
    "    # Choose the class with the highest posterior probability\n",
    "    predicted_class = torch.argmax(log_posterior)\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Example of predicting a new sample\n",
    "predicted_class = predict(xtest, cmean, cvar, yclass_count, yclass)\n",
    "print(\"Test sample:\", xtest)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorchifying Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "\n",
    "    def fit(self, xtrain, ytrain):\n",
    "        self.cls_value, self.cls_count = torch.unique(ytrain, return_counts=True)\n",
    "        self.yclass, self.yclass_count = torch.unique(ytrain, return_counts=True)\n",
    "        self.n_class = self.yclass.shape[0]  # Number of unique classes (2 in this case: 0 and 1)\n",
    "        self.n_features = xtrain.shape[1]\n",
    "        self.cmean = torch.zeros([self.n_class, self.n_features])  # Mean of each feature for each class\n",
    "        self.cvar = torch.zeros([self.n_class, self.n_features])   # Variance of each feature for each class\n",
    "        self.ycls = torch.zeros(self.n_class)   \n",
    "\n",
    "        for i, cls in enumerate(self.yclass):\n",
    "            x_cls = xtrain[ytrain == cls]  # Get all samples corresponding to the class\n",
    "            self.cmean[i] = x_cls.mean(dim=0)  # Mean of features for this class\n",
    "            self.cvar[i] = x_cls.var(dim=0, unbiased=False)  # Variance of features for this class\n",
    "            self.ycls[i] = self.cls_count[i].item() / ytrain.shape[0]  # Class prior P(class)\n",
    "        \n",
    "          # Returning self allows method chaining\n",
    "         \n",
    "\n",
    "    def predict(self, xtest):\n",
    "        self.n_classes = self.cmean.shape[0]\n",
    "        log_posterior = torch.zeros(self.n_classes)\n",
    "\n",
    "        # Compute log likelihood and log prior for each class\n",
    "        for i in range(self.n_classes):\n",
    "            # Calculate the normal PDF for each feature in the test sample for class i\n",
    "            log_likelihood = torch.sum(log(norm_pdf(xtest, self.cmean[i], self.cvar[i])))  # Sum of log(P(x_i | class))\n",
    "            log_prior = log(self.yclass_count[i])  # Log(P(class))\n",
    "            \n",
    "            # Total log posterior (log of P(x | class) * P(class))\n",
    "            log_posterior[i] = log_likelihood + log_prior\n",
    "\n",
    "        # Choose the class with the highest posterior probability\n",
    "        predicted_class = torch.argmax(log_posterior)\n",
    "        return predicted_class.item()\n",
    "        \n",
    "\n",
    "    def norm_pdf(self, xtest, mu, var):\n",
    "        return torch.exp(-0.5 * ((xtest - mu) ** 2) / var) / (2 * 3.14 * var) ** 0.5\n",
    "         \n",
    "    def log(z):\n",
    "        return torch.log(z)\n",
    "     \n",
    "\n",
    "\n",
    "obj = NaiveBayes()\n",
    "obj1 = obj.fit(x,y)\n",
    "cls_predict =  obj.predict(xtest)\n",
    "cls_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

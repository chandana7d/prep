{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **1. Combined Classifier Prediction**\n",
    "The final strong classifier \\(H(x)\\) combines the predictions of \\(M\\) weak learners, weighted by their importance (\\(\\alpha_m\\)):\n",
    "$$\n",
    "H(x) = \\text{sign}\\left(\\sum_{m=1}^M \\alpha_m \\cdot h_m(x)\\right).\n",
    "$$\n",
    "Inference is considered from the sign of H(x) where all the multiple weak learners are used \n",
    "#### **2. Exponential Loss Function**\n",
    "AdaBoost minimizes the exponential loss function, which assigns higher penalties to misclassified samples:\n",
    "$$\n",
    "L = \\sum_{i=1}^n w_i \\cdot \\exp(-y_i \\cdot H(x_i)),\n",
    "$$\n",
    "where:\n",
    "- \\(w_i\\): Weight of sample \\(i\\),\n",
    "- \\(y_i\\): True label of sample \\(i\\),\n",
    "- \\(H(x_i)\\): Combined prediction from all weak learners for sample \\(i\\).\n",
    "\n",
    "#### **3. Minimizing the Loss**\n",
    "To find the weight of the current weak learner (\\(\\alpha_m\\)), the exponential loss is minimized with respect to \\(\\alpha_m\\):\n",
    "$$\n",
    "\\alpha_m = 0.5 \\cdot \\ln\\left(\\frac{1 - \\text{error}}{\\text{error}}\\right),\n",
    "$$\n",
    "where \\(\\text{error}\\) is the weighted classification error of the weak learner:\n",
    "$$\n",
    "\\text{error} = \\frac{\\sum_{i=1}^n w_i \\cdot \\mathbb{I}(h_m(x_i) \\neq y_i)}{\\sum_{i=1}^n w_i}.\n",
    "$$\n",
    "\n",
    "#### **4. Sample Weights Update**\n",
    "After determining \\(\\alpha_m\\), the sample weights are updated to emphasize misclassified samples:\n",
    "$$\n",
    "w_i \\leftarrow w_i \\cdot \\exp(-\\alpha_m \\cdot y_i \\cdot h_m(x_i)).\n",
    "$$\n",
    "- **Correctly classified samples**: \\(y_i \\cdot h_m(x_i) = +1\\), weight decreases.\n",
    "- **Misclassified samples**: \\(y_i \\cdot h_m(x_i) = -1\\), weight increases.\n",
    "\n",
    "#### **5. Normalization**\n",
    "To ensure the weights remain a valid probability distribution:\n",
    "$$\n",
    "w_i \\leftarrow \\frac{w_i}{\\sum_{j=1}^n w_j}.\n",
    "$$\n",
    "This ensures:\n",
    "$$\n",
    "\\sum_{i=1}^n w_i = 1.\n",
    "$$\n",
    "\n",
    "\n",
    "### Final Notes\n",
    "- AdaBoost iteratively builds a strong classifier by focusing on harder-to-classify samples through weight adjustments.\n",
    "- The combined classifier \\(H(x)\\) is a weighted majority vote of the weak learners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### *** Derivation: Minimize the Loss**\n",
    "To find the weight of the current weak learner (\\(\\alpha_m\\)), the exponential loss is minimized with respect to \\(\\alpha_m\\):\n",
    "\n",
    "**Step 1: Take the derivative of \\(L_m\\) with respect to \\(\\alpha_m\\):**\n",
    "$$\n",
    "\\frac{\\partial L_m}{\\partial \\alpha_m} = -W_{\\text{correct}} \\cdot \\exp(-\\alpha_m) + W_{\\text{misclassified}} \\cdot \\exp(+\\alpha_m) = 0,\n",
    "$$\n",
    "where:\n",
    "- \\(W_{\\text{correct}}\\): Sum of weights for correctly classified samples,\n",
    "- \\(W_{\\text{misclassified}}\\): Sum of weights for misclassified samples.\n",
    "\n",
    "**Step 2: Rearrange terms:**\n",
    "$$\n",
    "W_{\\text{correct}} \\cdot \\exp(-\\alpha_m) = W_{\\text{misclassified}} \\cdot \\exp(+\\alpha_m).\n",
    "$$\n",
    "\n",
    "**Step 3: Divide through by \\(\\exp(-\\alpha_m)\\):**\n",
    "$$\n",
    "W_{\\text{correct}} = W_{\\text{misclassified}} \\cdot \\exp(2\\alpha_m).\n",
    "$$\n",
    "\n",
    "**Step 4: Solve for \\(\\exp(2\\alpha_m)\\):**\n",
    "$$\n",
    "\\exp(2\\alpha_m) = \\frac{W_{\\text{correct}}}{W_{\\text{misclassified}}}.\n",
    "$$\n",
    "\n",
    "**Step 5: Take the natural logarithm:**\n",
    "$$\n",
    "2\\alpha_m = \\ln\\left(\\frac{W_{\\text{correct}}}{W_{\\text{misclassified}}}\\right).\n",
    "$$\n",
    "\n",
    "**Step 6: Simplify:**\n",
    "$$\n",
    "\\alpha_m = 0.5 \\cdot \\ln\\left(\\frac{1 - \\text{error}}{\\text{error}}\\right),\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "\\text{error} = \\frac{W_{\\text{misclassified}}}{W_{\\text{total}}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    #node consists of feature, which threshold it is split on, child node left and right childe \n",
    "    # node and if it is a leaf value of the leaft node i.e class of the leaf node\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    #if value is not none then it is a leaf node will be appened inthe training state and used for interference purpose    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "grow tree \n",
    "best split\n",
    "information gain\n",
    "entropy \n",
    "'''\n",
    "\n",
    "class DecisionTree:\n",
    "    #splitting and stopping critieria - min samples in the node to split , max layer depth of the tree, total number of features in the tree both splitting and \n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # check the stopping criteria\n",
    "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # calculate the information gain\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        # calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        # Count occurrences of each class\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        ps = counts / len(y)  # Probabilities\n",
    "        return -np.sum([p * np.log(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators = 50):\n",
    "        self.n_estimators =n_estimators\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        n_samples, _ = X.shape\n",
    "        weights = np.ones(n_samples)/n_samples\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "\n",
    "            #builing a weak learner decision tree stump \n",
    "            tree = DecisionTree(max_depth = 1)\n",
    "            tree.fit(X,y)\n",
    "\n",
    "            y_pred = tree.predict(X)\n",
    "\n",
    "            #calculating the weighted loss  of this model predictions \n",
    "            error = np.sum(weights*(y_pred !=y))/np.sum(weights)\n",
    "\n",
    "            #calculaitng this model weight alpha this is derived from above and \n",
    "            #this deriviation is from by minizing the exponential loss function consider for this \n",
    "\n",
    "            alpha = 0.5 * np.log( (1- error)/(error+1e-10 )) #to get ride of zero divison error\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(tree)\n",
    "\n",
    "            #updatating the weights this is the sample weights \n",
    "            #simply assinging the exponential term for the weights \n",
    "            #high weightage for more misclasifed predications and less weight assinged for rightly classifeid \n",
    "            weights *= np.exp(-alpha * y * y_pred) \n",
    "            weights /= np.sum(weights) #normlizign the updated weights to sum 1 \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        #build the emply pred array of n samples \n",
    "        pred = np.zeros(X.shape[0])\n",
    "\n",
    "        #finding H = a1*h1 + a2*h2 +.....+am*hm for m - boosted m weak samples \n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            pred += alpha * model.predict(X)\n",
    "        \n",
    "        return np.sign(pred)  #returns +1 or -1 from the classification \n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a simple dataset\n",
    "X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n",
    "y = np.where(y == 0, -1, 1)  # Convert labels to -1 and 1 for binary classification\n",
    "\n",
    "# Split into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# AdaBoost\n",
    "adaboost = AdaBoost(n_estimators=50)\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred_ada = adaboost.predict(X_test)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_ada))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2., nan],\n",
      "        [ 4., nan,  6.],\n",
      "        [nan,  8.,  9.],\n",
      "        [10., 11., 12.],\n",
      "        [nan, nan, nan]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a PyTorch tensor with missing data (NaN) for testing\n",
    "missing_data_tensor = torch.tensor([\n",
    "    [1.0, 2.0, float('nan')],\n",
    "    [4.0, float('nan'), 6.0],\n",
    "    [float('nan'), 8.0, 9.0],\n",
    "    [10.0, 11.0, 12.0],\n",
    "    [float('nan'), float('nan'), float('nan')]\n",
    "])\n",
    "\n",
    "print(missing_data_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ignoring or Dropping Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Missing Data\n",
    "\n",
    "Dealing with missing data is a critical step in data preprocessing and analysis. Here are various methods to address missing data, categorized into three main approaches:\n",
    "\n",
    "| **Category**                 | **Method**                                                                                  | **Description**                                                                                                                                   | **Pros**                                | **Cons**                                              |\n",
    "|------------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------|-------------------------------------------------------|\n",
    "| **1. Ignoring or Dropping**  | **Listwise Deletion**                                                                       | Remove rows where any value is missing.                                                                                                           | Simple to implement.                    | Loss of data.                                         |\n",
    "|                              | **Column-Wise Deletion**                                                                    | Drop columns with a high proportion of missing values.                                                                                           | Useful for irrelevant columns.          | Loss of features.                                     |\n",
    "| **2. Statistical Imputation**| **Mean/Median/Mode Imputation**                                                             | Replace missing values with the mean, median, or mode of the column.                                                                             | Quick and easy.                          | Reduces variability in data.                         |\n",
    "|                              | **Forward/Backward Fill**                                                                   | Fill missing values using the previous or next observations.                                                                                     | Useful in time series data.             | Assumes data continuity.                             |\n",
    "|                              | **Random Imputation**                                                                       | Replace missing values with random values from the column.                                                                                       | Preserves data distribution.             | Adds randomness.                                     |\n",
    "| **2. Predictive Imputation** | **Regression Imputation**                                                                   | Use regression models to predict missing values based on other variables.                                                                        | More accurate.                           | Computationally expensive.                           |\n",
    "|                              | **k-Nearest Neighbors (KNN)**                                                               | Replace missing values with the average or mode of the k-nearest neighbors.                                                                      | Captures variable relationships.         | Computationally intensive for large datasets.        |\n",
    "|                              | **Multivariate Imputation by Chained Equations (MICE)**                                     | Impute data by iteratively modeling each variable based on the others.                                                                           | Effective for complex datasets.          | Computationally expensive.                           |\n",
    "| **2. Advanced Techniques**   | **Deep Learning or ML Models**                                                             | Use models like random forests or neural networks to predict missing values.                                                                     | Captures complex patterns.               | Requires significant resources.                      |\n",
    "|                              | **Multiple Imputation**                                                                     | Generate multiple datasets, analyze, and combine results.                                                                                        | Captures uncertainty in imputations.     | More complex process.                                |\n",
    "|                              | **Domain-Specific Imputation**                                                              | Use heuristics or domain rules to fill missing values.                                                                                           | Leverages domain knowledge.              | Requires domain expertise.                           |\n",
    "| **3. Transformation**        | **Create Missing Indicator**                                                               | Add a binary variable indicating missing data.                                                                                                   | Preserves missingness information.       | Adds dimensionality.                                 |\n",
    "|                              | **Replace with Constant**                                                                   | Replace missing values with a constant (e.g., `0`, `-1`).                                                                                        | Simple to implement.                     | May distort interpretation.                          |\n",
    "|                              | **Binning**                                                                                 | Transform continuous variables into categorical bins.                                                                                            | Useful for specific cases.               | Loss of granularity.                                 |\n",
    "| **4. Data Integration**      | **External Data Sources**                                                                   | Use additional data to infer missing values.                                                                                                     | Can improve accuracy.                    | Limited by availability of external data.            |\n",
    "|                              | **Data Augmentation**                                                                       | Generate synthetic data points to supplement missing data.                                                                                       | Can enhance datasets.                    | May introduce noise or bias.                         |\n",
    "| **5. Model-Specific Handling**| **Model Tolerance**                                                                         | Use algorithms that handle missing data natively (e.g., XGBoost, LightGBM).                                                                      | No preprocessing required.               | Limited to specific algorithms.                      |\n",
    "|                              | **Use Data Directly**                                                                       | Use models capable of handling missing values without transformation.                                                                            | Simplifies workflow.                     | May not work for all models.                         |\n",
    "| **6. Analysis Adaptation**   | **Sensitivity Analysis**                                                                    | Test how imputation methods affect results.                                                                                                      | Ensures robust results.                  | Requires additional analysis.                        |\n",
    "|                              | **Weighting or Reweighting**                                                                | Adjust weights for missing observations based on known distributions.                                                                            | Maintains overall accuracy.              | Requires distributional assumptions.                 |\n",
    "| **7. Do Nothing**            | **Leave Missing Values Unchanged**                                                         | Suitable for models that work with missing data directly (e.g., some deep learning models).                                                      | Simplifies process.                      | May reduce accuracy for some methods.                |\n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing the Right Method**\n",
    "\n",
    "The choice depends on:\n",
    "- **Data Size:** Large datasets tolerate listwise deletion better than small ones.\n",
    "- **Data Type:** Numerical or categorical data.\n",
    "- **Missingness Pattern:** MCAR, MAR, or MNAR.\n",
    "- **Domain Knowledge:** Contextual understanding of the problem.\n",
    "- **Analysis Goal:** Predictive accuracy, interpretability, or exploratory insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection and Handling Methods\n",
    "\n",
    "## Outlier Detection Methods\n",
    "\n",
    "| **Method**                       | **Description**                                                                 | **Used For**                                  |\n",
    "|-----------------------------------|---------------------------------------------------------------------------------|-----------------------------------------------|\n",
    "| **Z-Score / Standard Score**      | Measures how many standard deviations a data point is from the mean.            | Identifying outliers in normal distributions. |\n",
    "| **Modified Z-Score**              | Robust version using median and MAD instead of mean and standard deviation.     | Outliers in non-normally distributed data.    |\n",
    "| **IQR (Interquartile Range)**     | Detects outliers based on the range between the first and third quartiles.      | Numerical data.                              |\n",
    "| **Euclidean Distance**            | Measures distance between data points; far distances suggest outliers.          | General use, often in clustering.            |\n",
    "| **k-NN (k-Nearest Neighbors)**    | Detects outliers by checking the distance to the k-th nearest neighbor.         | High-dimensional datasets.                   |\n",
    "| **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** | Identifies points with low density as outliers.                                | Clustering-based outlier detection.          |\n",
    "| **Isolation Forest**              | Uses trees to isolate outliers. The fewer splits required, the more likely itâ€™s an outlier. | High-dimensional data.                       |\n",
    "| **One-Class SVM**                 | Identifies outliers by fitting a decision boundary around the majority of data points. | Unsupervised learning for anomaly detection. |\n",
    "| **LOF (Local Outlier Factor)**    | Measures density deviation of a point with respect to its neighbors.           | Local outliers in density-based datasets.     |\n",
    "| **K-Means Clustering**            | Identifies outliers by calculating the distance from cluster centroids.        | General use.                                 |\n",
    "| **Gaussian Mixture Models (GMM)** | Assumes data points follow a mixture of Gaussian distributions. Points with low probabilities are outliers. | Probabilistic clustering.                   |\n",
    "| **Agglomerative Clustering**      | Detects outliers by examining points that form singleton clusters.              | Clustering-based detection.                  |\n",
    "| **Box Plot**                      | Visual method using quartiles to detect outliers as points outside the whiskers. | Simple visual detection.                     |\n",
    "| **Scatter Plot**                  | Identifies outliers by visualizing data points far from the main concentration. | Multivariate data.                           |\n",
    "| **Histogram**                     | Visualizes outliers as bins far from the main distribution.                    | Simple visualization.                        |\n",
    "\n",
    "## Outlier Handling Methods\n",
    "\n",
    "| **Method**                        | **Description**                                                               | **Use Case**                                  |\n",
    "|------------------------------------|-------------------------------------------------------------------------------|-----------------------------------------------|\n",
    "| **Remove Outliers**                | Remove data points identified as outliers based on detection methods.         | When outliers are erroneous or not useful.    |\n",
    "| **Replace with Mean/Median/Mode**  | Replace outliers with the mean, median, or mode of the feature.               | When outliers are suspected to be errors.     |\n",
    "| **Impute using KNN**               | Use k-nearest neighbors to replace outliers with values based on similar data. | When outliers need to be replaced with plausible values. |\n",
    "| **Log Transformation**             | Apply a logarithmic transformation to reduce the impact of extreme values.    | Skewed data with extreme values.              |\n",
    "| **Square Root or Cube Root**       | Apply square root or cube root to stabilize variance and reduce skewness.    | Skewed data with a small number of large values. |\n",
    "| **Box-Cox Transformation**         | A family of transformations to make data more normal-distributed and reduce outlier impact. | Non-normal data with outliers. |\n",
    "| **Clipping**                       | Replace outliers with predefined thresholds or percentiles (e.g., 95th percentile). | When outliers are reasonable but extreme.     |\n",
    "| **Robust Regression**              | Use models like RANSAC that are less sensitive to outliers.                   | When using regression models.                 |\n",
    "| **Robust PCA**                     | PCA method that reduces the influence of outliers in dimensionality reduction. | High-dimensional data with outliers.          |\n",
    "| **Cluster Outliers**               | Treat outliers as a separate cluster if they represent valuable but rare events. | Fraud detection, anomaly detection.           |\n",
    "| **Ensemble Methods (e.g., Random Forests)** | Combine multiple decision trees to reduce the impact of outliers.            | When using tree-based algorithms.             |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
